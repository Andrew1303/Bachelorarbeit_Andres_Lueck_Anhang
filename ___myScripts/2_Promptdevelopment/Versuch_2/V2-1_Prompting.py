import json
import time
import chromadb
from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings
import numpy as np
from Fct_LLM_API import ChattingOne

# DESCRIPTION
# ! This script requires precalculated embeddings for prompts and knowledge base. !
# It is made to slowly go over all given prompts with embeddings, find the right context and send it to an llm which is generating an answer for the prompt.
# The user of this script is able to look into all human relevant information to check the functionality of the RAG (Retrieval Augmented Generation) system and the answer generated by the selected llm.

# -- Get embeddings precalculated --
# Open client
client = chromadb.PersistentClient(
    path=r"__Recherche\RAG\Skripts\chromadb",
    settings=Settings(),
    tenant=DEFAULT_TENANT,
    database=DEFAULT_DATABASE
)

# Load the collection from the client
collection = client.get_collection("gte-Qwen2-1.5B-instruct_MIL-HDBK-217F_lenchunk-1000_ol-0.3_V1")

# Get the data of the selected collection
result = collection.get(include=["documents", "embeddings"])
embeddings = result["embeddings"]

# -- Get prompts with precalculated embedding --
# Open file with prompts and embeddings
with open("___myScripts\\2_Promptdevelopment\\Versuch_2\\GeneratedPromptsWithEmbeedings\\TEST.json") as jsondata:
    prompts = json.load(jsondata)

# Request llm for each prompt and embedding
for language in prompts:
    for id in prompts[f"{language}"]:
        # Available info: prompt, embedding

        promptembedding = prompts[language][id]["embedding"]
        scores = (promptembedding @ np.array(embeddings).T) * 100
        scores = scores.tolist()
        highest_score_index = scores.index(max(scores))
        context = result["documents"][highest_score_index]
        print(f"\033[1;34m{prompts[language][id]["prompt"]}\033[0m")
        showcontext = input('Send "y" to show context? (Enter to skip)')
        if showcontext == "y":
            print(context)

        # Request with prompt and context
        start = time.time()
        finalresult = ChattingOne(f"{prompts[f"{language}"][f"{id}"]["prompt"]}", context, "llama3.2-vision:latest", temperature=0.2)
        end = time.time()
        duration = end - start
        print(finalresult)
        print(f'Duration: {duration}')
        input("Continue: Press Enter")